<p align="center">
    <a href="https://www.perplexity.ai/" title="Go to Perplexity homepage" target="_blank">
        <img src="https://img.shields.io/badge/PERPLEXITY%20AI-20808d?style=for-the-badge&logo=data:image/png;base64,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" alt="Perplexity AI">
    </a>
</p>

<p align="center">
    <a href=".github/version.json" title="Go to changelog" target="_blank">
        <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&label=Perplexity+AI+Wrapper+and+CLI&query=version&url=https%3A%2F%2Fraw.githubusercontent.com%2FRMNCLDYO%2FPerplexity-AI-Wrapper-and-CLI%2Fmain%2F.github%2Fversion.json" alt="Version">
    </a>
</p>

<p align="center">
    <a href=".github/CHANGELOG.md" title="Go to changelog" target="_blank"><img src="https://img.shields.io/badge/maintained-yes-2ea44f?style=for-the-badge" alt="maintained - yes"></a>
    <a href=".github/CONTRIBUTING.md" title="Go to contributions doc" target="_blank"><img src="https://img.shields.io/badge/contributions-welcome-2ea44f?style=for-the-badge" alt="contributions - welcome"></a>
</p>

<p align="center">
    <a href="/">
        <img width="700" src="https://raw.githubusercontent.com/RMNCLDYO/Perplexity-AI-Wrapper-and-CLI/main/.github/logo.png">
    </a>
</p>

## Overview

A Python Wrapper and Command-Line Interface (CLI) for Perplexity AI, designed to facilitate seamless interaction with the full suite of language models offered by Perplexity Labs. 

The tool was crafted to support both individuals seeking direct access to AI-powered chat and search capabilities through the command line and developers looking to integrate AI functionalities into their applications. Whether you're executing web searches or engaging in conversations with AI, this tool streamlines the process, making advanced AI accessible to everyone.

## Features

- **Command-Line Interface (CLI)**: Search online (in real-time) or engage in conversational chats (similar to ChatGPT) directly from the terminal.
- **Python Wrapper**: Enables seamless interaction with the full suite of AI models offered by Perplexity Labs using just two lines of code.
- **Diverse Model Selection**: Choose from an extensive range of models including `codellama-70b-instruct`, `pplx-7b-chat`, `pplx-70b-chat`, `pplx-7b-online`, `pplx-70b-online`, `llama-2-70b-chat`, `codellama-34b-instruct`, `mistral-7b-instruct`, and `mixtral-8x7b-instruct`, optimized for specific tasks like conversation and web search, ensuring the right fit for every project.
- **Customizable AI Experience**: Tailor AI behavior to your exact needs with adjustable settings for model choice, response style, output length, and more, granting unparalleled access and control a variety of large language models.

## Prerequisites

- Python 3.x
- A Perplexity AI account and API key.

## Dependencies
The following Python packages are required:
- `requests`: For making HTTP requests to the Perplexity API.

The following Python packages are optional:
- `python-dotenv`: For managing API keys and other environment variables.

## Installation

1. Clone the repository:
    ```bash
    git clone https://github.com/RMNCLDYO/Perplexity-AI-Wrapper-and-CLI.git
    cd Perplexity-AI-Wrapper-and-CLI
    ```

2. Install the dependencies:
    ```bash
    pip install -r requirements.txt
    ```

## Getting Started

### Obtaining an API Key

> [!IMPORTANT]  
> In order to generate an API key for Perplexity you must first add your payment information to your Perplexity account. API keys can only be generated when your balance is nonzero.

1. Sign up for an account at [Perplexity AI](https://perplexity.ai).
2. Add your payment information to your Perplexity account.
3. Navigate to `</> API` in your account settings or access its directly [here](https://www.perplexity.ai/pplx-api).
4. Click `+ Generate` to create your API Key.
5. Copy your API key and store it somewhere safe.

### Configuration (*Optional*)

> [!WARNING]  
> If you choose not to add your API key to an `.env` file, then you must pass your API key to the wrapper or CLI when running the script.

1. Once you have your API key, you can create a new file named `.env` in the root directory (main folder) of this project, or rename the `example.env` file in the root directory of this project to `.env`.
2. You can then add your API key to the `.env` file as follows:
   ```
   API_KEY=your_api_key_here
   ```
3. The program will automatically load and use your API key whenever you use this tool.

## General Usage

### CLI Tool

#### Online Search Session
```bash
python pplx_cli.py search --query "What is today's date?"
```

#### Chat Session
```bash
python pplx_cli.py chat
```

#### Help
```bash
python pplx_cli.py --help
```

### Python Wrapper

#### Online Search Session
```python
from pplx_search import SearchAPI

SearchAPI().search(query="What is today's date?")
```

> An executable version of this example can be found [here](./examples/example_search.py)

#### Chat Session

```python
from pplx_chat import ChatAPI

ChatAPI().chat()
```

> An executable version of this example can be found [here](./examples/example_chat.py)

## Advanced Configuration

This section provides a comprehensive overview of each configuration option available through the command-line and wrapper interfaces, including their cli flags, example cli and wrapper usage, parameter limits, and detailed descriptions of each option to guide you in customizing your interactions with the full suite of AI models offered by Perplexity Labs.

### CLI Options
| Option(s)                    | Description                                     | Example Usage                                  |
|------------------------------|-------------------------------------------------|------------------------------------------------|
| `chat`                       | Start a new chat session.                       | `chat`                                         |
| `search`                     | Start a new search session with a query.        | `search --query "Your search query"`           |
| `-a`, `--api_key`            | Your Perplexity API key.                        | `--api_key your_api_key`                       |
| `-q`, `--query`              | Your online search query.                       | `--search --query "Your search query"`         |
| `-m`, `--model`              | Select the model for your session.              | `--model "pplx-70b-chat"`                        |
| `-st`, `--stream`            | Enable streaming responses.                     | `--stream`                                     |
| `-sp`, `--system_prompt`     | Set an initial system prompt.                   | `--system_prompt "The initial system prompt"`  |
| `-mt`, `--max_tokens`        | Set the maximum number of response tokens.      | `--max_tokens 100`                             |
| `-t`, `--temperature`        | Adjust the randomness of the response.          | `--temperature 0.7`                            |
| `-tp`, `--top_p`             | Set nucleus sampling threshold.                 | `--top_p 0.9`                                  |
| `-tk`, `--top_k`             | Number of top tokens to consider for filtering. | `--top_k 40`                                   |
| `-pp`, `--presence_penalty`  | Penalize new tokens based on their presence.    | `--presence_penalty 0.5`                       |
| `-fp`, `--frequency_penalty` | Penalize new tokens based on their frequency.   | `--frequency_penalty 0.5`                      |

### Wrapper Options
| Option(s)           | Description                                     | Example Usage                               |
|---------------------|-------------------------------------------------|---------------------------------------------|
| `api_key`           | Your Perplexity API key.                        | `api_key="your_api_key"`                    |
| `query`             | Your query for the online search model.         | `query="Your search query"`                 |
| `model`             | Select the model for your session.              | `model="pplx-70b-chat"`                      |
| `stream`            | Enable streaming responses.                     | `stream=True`                               |
| `system_prompt`     | Set an initial system prompt.                   | `system_prompt="The initial system prompt"` |
| `max_tokens`        | Set the maximum number of response tokens.      | `max_tokens=100`                            |
| `temperature`       | Adjust the randomness of the response.          | `temperature=0.7`                           |
| `top_p`             | Set nucleus sampling threshold.                 | `top_p=0.9`                                 |
| `top_k`             | Number of top tokens to consider for filtering. | `top_k=40`                                  |
| `presence_penalty`  | Penalize new tokens based on their presence.    | `presence_penalty=0.5`                      |
| `frequency_penalty` | Penalize new tokens based on their frequency.   | `frequency_penalty=0.5`                     |

### Usage Details
| Option(s)           | Details                                                                                                      |
|---------------------|--------------------------------------------------------------------------------------------------------------|
| `chat`              | Initiates a chat session with the specified model. *(CLI ONLY)*                                              |
| `search`            | Begins a web search session using the provided query string. *(CLI ONLY)*                                    |
| `api_key`           | Required for authenticating API requests. Your unique API key can be obtained from your Perplexity account.  |
| `query`             | Specifies the search query for web search sessions. Only applicable with the --search flag.                  |
| `model`             | The name of the model that will complete your prompt. Possible values include `codellama-70b-instruct`, `pplx-7b-chat`, `pplx-70b-chat`, `pplx-7b-online`, `pplx-70b-online`, `llama-2-70b-chat`, `codellama-34b-instruct`, `mistral-7b-instruct`, and `mixtral-8x7b-instruct`.                   |
| `stream`            | Enabling this feature will deliver the response in incremental segments, providing users with a continuous flow of data, akin to the way services like ChatGPT transmit information.                         |
| `system_prompt`     | The initial system prompt. The system prompt explicitly sets the instructions for the model.                                                   |
| `max_tokens`        | The maximum number of completion tokens returned by the API. The total number of tokens requested in max_tokens plus the number of prompt tokens sent in messages must not exceed the context window token limit of model requested. If left unspecified, then the model will generate tokens until either it reaches its stop token or the end of its context window.         |
| `temperature`       | The amount of randomness in the response, valued between 0 inclusive and 2 exclusive. Higher values are more random, and lower values are more deterministic. You should either set temperature or top_p, but not both.         |
| `top_p`             | The nucleus sampling threshold, valued between 0 and 1 inclusive. For each subsequent token, the model considers the results of the tokens with top_p probability mass. You should either alter temperature or top_p, but not both. |
| `top_k`             | The number of tokens to keep for highest top-k filtering, specified as an integer between 0 and 2048 inclusive. If set to 0, top-k filtering is disabled.                     |
| `presence_penalty`  | A value between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Incompatible with `frequency_penalty`.                                        |
| `frequency_penalty` | A multiplicative penalty greater than 0. Values greater than 1.0 penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. A value of 1.0 means no penalty. Incompatible with `presence_penalty`.                              |                                                               |

## Advanced Usage

### CLI Tool

#### Online Search Session *with Advanced Settings*
```bash
python pplx_cli.py search --api_key "YOUR_API_KEY_HERE" --query "What is today's date?" --stream
```

#### Chat Session *with Advanced Settings*
```bash
python pplx_cli.py chat --api_key "YOUR_API_KEY_HERE" --model "llama-2-70b-chat" --system_prompt "You are a comedian. All of your responses should be funny." --stream
```

#### Help
```bash
python pplx_cli.py --help
```

### Python Wrapper

#### Online Search Session *with Advanced Settings*
```python
from pplx_search import SearchAPI

SearchAPI().search(api_key="YOUR_API_KEY_HERE", query="What is today's date?", stream=True)
```

#### Chat Session *with Advanced Settings*

```python
from pplx_chat import ChatAPI

ChatAPI().chat(api_key="YOUR_API_KEY_HERE", model="llama-2-70b-chat", system_prompt="You are a comedian. All of your responses should be funny.", stream=True)
```

## Available Models

| Model                    | Context Length (*max tokens*)   |
|--------------------------|---------------------------------|
| `codellama-34b-instruct` | 16384                           |
| `codellama-70b-instruct` | 16384                           |
| `llama-2-70b-chat`       | 4096                            |
| `mistral-7b-instruct`    | 8192                            |
| `mixtral-8x7b-instruct`  | 8192                            |
| `pplx-7b-chat`           | 8192                            |
| `pplx-70b-chat`          | 8192                            |
| `pplx-7b-online`         | 8192                            |
| `pplx-70b-online`        | 8192                            |

**Last updated February 19, 2024*

## API Rate Limits

| Model                  | Request rate limit                         | Token rate limit                                         |
|------------------------|--------------------------------------------|----------------------------------------------------------|
| mistral-7b-instruct    | - 10/5seconds<br>- 50/minute<br>- 500/hour | - 8000/10seconds<br>- 80000/minute<br>- 256000/10minutes |
| mixtral-8x7b-instruct  | - 4/5seconds<br>- 12/minute<br>- 120/hour  | - 8000/minute<br>- 32000/10minutes                       |
| codellama-34b-instruct | - 10/5seconds<br>- 30/minute<br>- 300/hour | - 20000/minute<br>- 80000/10minutes                      |
| codellama-70b-instruct | - 10/5seconds<br>- 30/minute<br>- 300/hour | - 20000/minute<br>- 80000/10minutes                      |
| llama-2-70b-chat       | - 4/5seconds<br>- 12/minute<br>- 120/hour  | - 8000/minute<br>- 32000/10minutes                       |
| pplx-7b-chat           | - 4/5seconds<br>- 12/minute<br>- 120/hour  | - 8000/minute<br>- 32000/10minutes                       |
| pplx-70b-chat          | - 4/5seconds<br>- 12/minute<br>- 120/hour  | - 8000/minute<br>- 32000/10minutes                       |
| pplx-7b-online         | - 10/minute                                | N/A                                                      |
| pplx-70b-online        | - 10/minute                                | N/A                                                      |

**Last updated February 19, 2024*

## Contributing
Contributions are welcome!

Please refer to [CONTRIBUTING.md](.github/CONTRIBUTING.md) for detailed guidelines on how to contribute to this project.

## Reporting Issues
Encountered a bug? We'd love to hear about it. Please follow these steps to report any issues:

1. Check if the issue has already been reported.
2. Use the [Bug Report](.github/ISSUE_TEMPLATE/bug_report.md) template to create a detailed report.
3. Submit the report [here](https://github.com/RMNCLDYO/Perplexity-AI-Wrapper-and-CLI/issues).

Your report will help us make the project better for everyone.

## Feature Requests
Got an idea for a new feature? Feel free to suggest it. Here's how:

1. Check if the feature has already been suggested or implemented.
2. Use the [Feature Request](.github/ISSUE_TEMPLATE/feature_request.md) template to create a detailed request.
3. Submit the request [here](https://github.com/RMNCLDYO/Perplexity-AI-Wrapper-and-CLI/issues).

Your suggestions for improvements are always welcome.

## Versioning and Changelog
Stay up-to-date with the latest changes and improvements in each version:

- [CHANGELOG.md](.github/CHANGELOG.md) provides detailed descriptions of each release.

## Security
Your security is important to us. If you discover a security vulnerability, please follow our responsible disclosure guidelines found in [SECURITY.md](.github/SECURITY.md). Please refrain from disclosing any vulnerabilities publicly until said vulnerability has been reported and addressed.

## License
Licensed under the MIT License. See [LICENSE](LICENSE) for details.
